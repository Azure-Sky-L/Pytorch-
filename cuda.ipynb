{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch 中以下数据结构分为 CPU 和 GPU 两个版本\n",
    "# Tensor、Variable(包括 Parameter)、nn.Module(包括常用的layer、loss function, 以及容器 Sequential等)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch as t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.3.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mzl/virtualenv0.3/venv/lib/python3.6/site-packages/torch/cuda/__init__.py:97: UserWarning: \n",
      "    Found GPU0 GeForce GTX 950M which is of cuda capability 5.0.\n",
      "    PyTorch no longer supports this GPU because it is too old.\n",
      "    \n",
      "  warnings.warn(old_gpu_warn % (d, name, major, capability[1]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = t.Tensor(3, 4)\n",
    "# 返回一个新的 tensor，保存在第1块GPU上，但原来的tensor并没有改变\n",
    "tensor.cuda(0)\n",
    "tensor.is_cuda #False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 不指定所使用的 GPU 设备，将默认使用第一快 GPU\n",
    "tensor = tensor.cuda()\n",
    "tensor.is_cuda # True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variable = t.autograd.Variable(tensor)\n",
    "variable.cuda()\n",
    "variable.is_cuda # True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module = nn.Linear(3, 4)\n",
    "module.cuda(device = 0)\n",
    "module.weight.is_cuda # True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VeryBigModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VeryBigModule, self).__init__()\n",
    "        self.GiantParameter1 = t.nn.Parameter(t.randn(100000, 20000)).cuda(0)\n",
    "        self.GiantParameter2 = t.nn.Parameter(t.randn(20000, 100000)).cuda(1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.GiantParameter1.mm(x.cuda(0))\n",
    "# 两个 Parameter 所占的内存空间都非常大，大概是 8GB，如果将这两个 Parameter 同时放在一个 GPU 上几乎会\n",
    "# 将显存占满，无法再进行任何其他运算。此时可以通过这种方式将不同的计算分布到不同的 GPU 中\n",
    "#        x = self.GiantParameter2.mm(x.cuda(1))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 关于使用 GPU 一些额建议：\n",
    "# 1. GPU 运算速度快，但运算量小，并不能体现出它的优势，因此一些简单的操作可以直接利用CPU完成\n",
    "# 2. 数据在 CPU 和 GPU 之间的传播会比较耗时，应当尽量避免\n",
    "# 3. 在进行低精度的计算时，可以考虑 HalfTensor，相比 FloatTensor 能节省一半的显存\n",
    "# 但需千万注意数值溢出的情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight', \n",
       "               1\n",
       "               3\n",
       "              [torch.cuda.FloatTensor of size 2 (GPU 0)])])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 注意：大部分的损失函数也属于 nn.Module\n",
    "\n",
    "# 交叉熵损失函数，带权重\n",
    "criterion = t.nn.CrossEntropyLoss(weight = t.Tensor([1, 3]))\n",
    "input = t.autograd.Variable(t.randn(4, 2)).cuda()\n",
    "target = t.autograd.Variable(t.Tensor([1, 0, 0, 1])).long().cuda()\n",
    "\n",
    "#下面这行会报错，因为 weight 未被转移至 GPU\n",
    "#loss = criterion(input, target)\n",
    "\n",
    "#这行也会报错\n",
    "criterion.cuda()\n",
    "#loss = criterion(input, target)\n",
    "\n",
    "criterion._buffers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 注：pytorch 不支持型号过低的显卡\n",
    "# 显卡如果过低，会报错\n",
    "# 除了调用对象 .cuda 方法外，还可以使用 torch.cuda.device 指定默认使用哪一块 GPU\n",
    "# 或使用 torch.set_default_tensor_type 使程序默认使用 GPU ，不需要手动调用 cuda\n",
    "# 如果未指定使用哪一块 GPU，默认使用 GPU 0\n",
    "x = t.cuda.FloatTensor(2, 3)\n",
    "# x.get_device() == 0\n",
    "y = t.FloatTensor(2, 3).cuda()\n",
    "# y.get_device() == 0\n",
    "\n",
    "#指定默认使用 GPU 0\n",
    "with t.cuda.device(0):\n",
    "    # 在 GPU 1 上构建 tensor\n",
    "    a = t.cuda.FloatTensor(2, 3)\n",
    "    \n",
    "    # 将 tensor 转移至 GPU 1\n",
    "    b = t.FloatTensor(2, 3).cuda()\n",
    "    print('66',a.get_device() == b.get_device() == 0)\n",
    "    \n",
    "    c = a + b\n",
    "    print('66',c.get_device() == 0)\n",
    "    \n",
    "    z = x + y\n",
    "    print('66',z.get_device() == 0)\n",
    "    \n",
    "    # 手动指定使用 GPU 0\n",
    "    d = t.randn(2, 3).cuda(0)\n",
    "    \n",
    "    print('66',d.get_device() == 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.set_default_tensor_type('torch.cuda.FloatTensor') \n",
    "# 指定默认 tensor 的类型为 GPU 上的 FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = t.ones(2, 3)\n",
    "a.is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可以通过 t.save(obj, file_name) 等方法保存任意可序列化的对象\n",
    "# 然后通过 obj = t.load(file_name) 方法加载保存的数据\n",
    "a = t.Tensor(3, 4)\n",
    "if t.cuda.is_available():\n",
    "    a = a.cuda(0) # 把 a 转为 GPU1 上的tensor\n",
    "    t.save(a, 'a.pth')\n",
    "    \n",
    "    # 加载为 b， 存储于 GPU1 上（因为保存时 tensor 就在 GPU1 上）\n",
    "    b = t.load('a.pth')\n",
    "    \n",
    "    # 加载为 c， 存储于 CPU\n",
    "    c = t.load('a.pth', map_location = lambda storage, loc: storage)\n",
    "    \n",
    "    # 加载为 d, 存储于 GPU0 上\n",
    "    d = t.load('a.pth', map_location = {'cuda:1':'cuda:0'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.set_default_tensor_type('torch.FloatTensor')\n",
    "from torchvision.models import AlexNet\n",
    "model = AlexNet()\n",
    "# module 的 state_dict 是一个字典\n",
    "model.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module 对象的保存与加载\n",
    "t.save(model.state_dict(), 'alexnet.pth')\n",
    "model.load_state_dict(t.load('alexnet.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = t.optim.Adam(model.parameters(), lr = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.save(optimizer.state_dict(), 'optimizer.pth')\n",
    "optimizer.load_state_dict(t.load('optimizer.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = dict(\n",
    "    optimizer = optimizer.state_dict(),\n",
    "    model = model.state_dict(),\n",
    "    info = u'模型和优化器的所有参数'\n",
    ")\n",
    "t.save(all_data, 'all.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = t.load('all.pth')\n",
    "all_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
